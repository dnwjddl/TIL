# 과적합 시 고려해야될 사항
- Data Augmentation
- Weight Regularization
- Weight Initialization
- Learning rate scheduler(+Early Stopping)
- Data Normalization
- Various Optimizer
- Batch Normalization(Dropout)

## Data Augmentation
- 데이터 량 늘림

```python
### 케라스 데이터 증강 ###
from tensorflow.keras.preprocessing.image import ImageDataGenerator
#datagen = ImageDataGenerator()
datagen = ImageDataGenerator(rotation_range = 10, horizontal_flip = True, zoom_range = 0.1)

### 파이토치 데이터 증강 ###
from torchvision import transforms
transforms.Compose([
				transforms.CenterCrop(10),
				transforms.ToTensor(),
				transforms.Rescale(256),
				transforms.RandomCrop(224),
])
```

## Weight Regularization
- L1 norm (```Lasso```)
- L2 norm (```Ridge```): 유클리드 거리
- **L2 norm**이 더 효능이 좋음
- L1 + L2 norm (```Elastic Net```)
```python
l2_model = keras.models.Sequential([
    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),
                       activation='relu', input_shape=(NUM_WORDS,)),
    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),
                       activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

# or
# penalty에 규제 작성, alpha에 규제 강도 작성
sgd = SGDClassifier(loss = 'log', penalty= 'l2', alpha = 0.001)
```

## Weight Initialization
신경망이 깊어질수록 분포가 한쪽으로 쏠릴 수 있어서 Gradient Vanishing이 나타날 수도 있음  
이를 대비하기 위한 가중치 초기화
| Xavier 초기화 | KamingHe 초기화 |
|:----------:|:----------:|
|활성화함수가 Sigmoid나 Tanh|활성화함수가 ReLU|
#### Xavier 초기화
```python
### 텐서플로우 Xavier 초기화 ###
W = tf.get_variable("W", shape=[784, 256],
           initializer=tf.contrib.layers.xavier_initializer())
           
### 케라스 Xavier 초기화 ###
model = Sequential()
model.add(Dense(50, kernel_initializer='he_normal'))

### 파이토치 Xavier 초기화 ###
torch.nn.init.xavier_uniform_(linear1.weight)
torch.nn.init.xavier_uniform_(linear2.weight)
torch.nn.init.xavier_uniform_(linear3.weight)

# xavier initialization
torch.nn.init.xavier_uniform_(linear1.weight)
torch.nn.init.xavier_uniform_(linear2.weight)
torch.nn.init.xavier_uniform_(linear3.weight)
```
#### He 초기화
uniform 분포를 따르는 경우랑 Normal 분포를 따르는 경우
```python
### 텐서플로우 Xavier 초기화 ###
# 01
W1 = tf.get_variable('W1', shape=[784, 256],
       initializer=tf.contrib.layers.variance_scaling_initializer())
# 02       
initializer = tf.contrib.layers.variance_scaling_initializer()
W1 = tf.Variable(initializer([784,256]))
```
#### Bias 초기화
bias의 경우 일반적으로 0으로 초기화하는 것이 효율적임

## Learning Rate Scheduler (+Early Stopping)
```python
tf.keras.callbacks.LearningRateScheduler(schedule, verbose=0)

## 학습
callback = tf.keras.callbacks.LearningRateScheduler(scheduler)
history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
                  epochs=15, callbacks=[callback], verbose=0)
```
## Data Normalization
```python
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
```

## Various Optimizer
- ```Adam``` optimizer로 많이 쓰임
![image](https://user-images.githubusercontent.com/72767245/106107190-fa23dc00-6189-11eb-8c9c-065e18577a0a.png)


## Batch Normaliztion(Drop out)
배치 정규화를 사용하면 학습을 빠르게 진행할 수 있으며, 초깃값에 영향을 덜 받게 된다  
활성화 값 분포가 적당히 퍼지도록 개선 > 원활한 학습을 위함
- BN과 Dropout은 비슷한 역할을 하며 Layer가 Deep해짐에 따라 Dropout보단 BN이 많이 사용된다.

```python
nn.BatchNorm2d(16)
nn.Dropout(0.2)
```


### 하이퍼파라미터 튜닝 (K-Fold 모델 학습)
- 딥러닝에서는 많이 사용하진 않지만, 머신러닝에선 꼭 필요함
- 교차검증을 할 시, 전처리가 그 안에 포함되어야 함
```python
### 케라스에서 K-Fold ###
from sklearn.model_selection import KFold

kfold = KFold(n_splits=5, shuffle = True) #n_splits 만큼 나눈다
for train, validation in kfold.split(x_data, y_data): #n_splits 만큼 반복
    hist = model.fit(datagen.flow(x_data, y_data, batch_size =32),                     
                    epochs = 60, 
                     steps_per_epoch = x_data.shape[0]//32, 
                     validation_data = (x_data[validation], y_data[validation]), 
                     callbacks = callbacks,
                     verbose = 1)
		     
### sklearn에서 교차 검증 ### 
# 성능평가 #
from sklearn.model_selection import cross_validate
sgd = SGDClassifier(loss='log', penalty='l2', alpha=0.001, random_state=42)
scores = cross_validate(sgd, x_train_all, y_train_all, cv=10)
```

# 차원의 저주
- 데이터셋의 특징이 많아지면, 각 특성인 하나의 차원 또한 증가
- 데이터의 차원이 증가할수록 데이터 공간의 부피가 기하급수적으로 증가하여, 데이터의 밀도는 차원이 증가할수록 희소해짐
- **머신러닝**: 데이터의 차원이 증가할수록 데이터 포인트 간의 거리 또한 증가하게 되므로, 이러한 데이터를 이용해 머신러닝 알고리즘을 학습하게 되면 모델이 복잡해짐
  - 데이터의 밀도가 높이질때까지 **학습 데이터의 크기 늘리기**
  - 차원 축소
    - Projection
    - manifold learning
    - PCA

# PCA(Principal Component Analysis)
PCA 데이터에 가장 가까운 초평면(hyperplane)을 구한 다음, 데이터를 이 초평면에 투영(projection) 시킴
- 초평면을 찾아야되는데 : 학습데이터셋에서 분산이 최대인 축(axis)을 찾음
- 첫번째 축과 직교하면서 분산이 최대인 두번째 축을 찾음
- i번째 축을 정의하는 **단위벡터**를 i번째 **주성분**이라함

**공분산**: 2개의 특성(또는 변수) 간의 상관정도


---

- PCA는 분포된 데이터들의 주성분(Pricipal component)를 찾아주는 방법
- 벡터화하여 데이터의 분포를 설명
- 데이터 하나 하나에 대한 성분을 분석하는 것이 아니라, 여러 데이터들이 모여 하나의 분포를 이룰때 **이 분포의 주성분을 분석**해주는 방법

----

- PCA는 2차원 데이터 집합에 대해 PCA를 수행하면 2개의 서로 수직인 주성분 벡터를 반환
- 3차원 점들에 대해 PCA를 수행하면 3개의 서로 수직인 주성분 벡터를 반환

**PCA를 통해 얻어진 주성분 벡터들은 서로 수직인 관게에 있다**
- PCA의 목적은 원데이터의 분산을 최대한 보존하는 축을 찾아 투영하는 것
---

### PCA가 영상인식에 활용되는 예 -> ```face recignition``` (```eigenface```)
- 이미지에서 픽셀 밝기값을 일렬로 연결하여 벡터로 만들면 얼굴이미지 벡터 생성
- 이 점들의 데이터들을 가지고 PCA를 수행하면 데이터의 차원수와 동일한 개수의 주성분 벡터들을 얻을 수 있음
- PCA를 통해 K개의 주요 eigenface들을 구한 후 각 개인들을 eigenface로 근사했을때의 근사계수를 구함
